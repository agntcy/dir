// Copyright AGNTCY Contributors (https://github.com/agntcy)
// SPDX-License-Identifier: Apache-2.0

syntax = "proto3";

package core.v1alpha1;

import "google/protobuf/struct.proto";

// LLM information provides metadata about the LLM (Large Language Model)
message LLMInfo {
  // Name of the LLM provider (e.g., "openai", "anthropic", "google", "azure", etc.)
  string provider = 1;

  // Version of the LLM model (e.g., "2024-03", "v2", etc.)
  string version = 2;

  // Model name or identifier (e.g., "gpt-4-turbo", "claude-3-opus", etc.)
  string model_name = 3;

  // Additional metadata associated with this LLM
  map<string, string> annotations = 4;

  // Model configuration
  LLMConfig config = 5;

  // Provider-specific endpoint information
  LLMEndpoint endpoint = 6;

  // Model capabilities and constraints
  LLMCapabilities capabilities = 7;
}

// Configuration for the LLM
message LLMConfig {
  // Temperature controls randomness in responses (0.0 to 1.0)
  float temperature = 1;

  // Maximum number of tokens to generate
  int32 max_tokens = 2;

  // Stop sequences that signal the end of generation
  repeated string stop_sequences = 3;

  // Top-p (nucleus) sampling parameter (0.0 to 1.0)
  float top_p = 4;

  // Frequency penalty for token generation (-2.0 to 2.0)
  float frequency_penalty = 5;

  // Presence penalty for token generation (-2.0 to 2.0)
  float presence_penalty = 6;

  // Response format specification (e.g., "json", "text")
  string response_format = 7;

  // Seed for deterministic responses
  optional int64 seed = 8;

  // Provider-specific configuration parameters
  google.protobuf.Struct provider_config = 9;
}

// Endpoint information for the LLM provider
message LLMEndpoint {
  // Base URL for API requests
  string base_url = 1;

  // API version to use
  string api_version = 2;

  // Custom headers required for API requests
  map<string, string> headers = 6;
}

// Model capabilities and constraints
message LLMCapabilities {
  // Maximum context length in tokens
  int32 max_context_tokens = 1;

  // Maximum response length in tokens
  int32 max_response_tokens = 2;

  // Supported features (e.g., "streaming", "function-calling")
  repeated string features = 3;

  // Supported response formats
  repeated string supported_formats = 4;

  // Model's primary strengths/focus areas
  repeated string specializations = 5;

  // Cost per token (in millicents, 1/1000th of a cent)
  float cost_per_1k_prompt_tokens = 6;
  float cost_per_1k_completion_tokens = 7;
}

// Usage tracking and quota information
message LLMQuota {
  // Number of requests per minute allowed
  int32 requests_per_minute = 1;

  // Number of tokens per minute allowed
  int32 tokens_per_minute = 2;

  // Maximum concurrent requests allowed
  int32 max_concurrent_requests = 3;

  // Quota reset period in seconds
  int32 quota_reset_period = 4;

  // Whether to enable quota tracking
  bool enable_quota_tracking = 5;
}
