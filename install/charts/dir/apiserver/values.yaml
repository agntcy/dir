# Copyright AGNTCY Contributors (https://github.com/agntcy)
# SPDX-License-Identifier: Apache-2.0

nameOverride: ""
fullnameOverride: ""

# Logging configuration
log_level: INFO
log_format: text # Options: "text" (development) or "json" (production)
grpc_logging_verbose: false # Options: false (production - logs start/finish only) or true (verbose - includes payloads)

# Enable coverage volume (emptyDir persists across container restarts)
coverageVolume: false

image:
  repository: ghcr.io/agntcy/dir-apiserver
  tag: latest
  pullPolicy: IfNotPresent
  pullSecrets: []

# Authorization policy file content (separate from config)
# This is written to authz_policies.csv file, not part of the server config struct
authz_policies_csv: |
  p,example.org,*
  p,*,/agntcy.dir.store.v1.StoreService/Pull
  p,*,/agntcy.dir.store.v1.StoreService/PullReferrer
  p,*,/agntcy.dir.store.v1.StoreService/Lookup
  p,*,/agntcy.dir.sync.v1.SyncService/RequestRegistryCredentials

# Server configuration (maps directly to server config.go Config struct)
config:
  # listen_address: "0.0.0.0:8888"

  # Authentication settings (handles identity verification)
  # Supports both X.509 (X.509-SVID) and JWT (JWT-SVID) authentication
  authn:
    # Enable authentication
    enabled: false
    # Authentication mode: "x509" or "jwt"
    # - x509: Uses X.509-SVID from mutual TLS peer certificates
    # - jwt: Uses JWT-SVID from Authorization header
    mode: "x509"
    # SPIFFE Workload API socket path (injected by SPIRE agent)
    socket_path: "unix:///run/spire/agent-sockets/api.sock"
    # Expected audiences for JWT validation (only used in JWT mode)
    audiences:
      - "spiffe://example.org/dir-server"

  # Authorization settings (handles access control policies)
  # Requires authentication to be enabled first
  authz:
    # Enable authorization policies
    enabled: false

    # Path to the authorization policy file
    enforcer_policy_file_path: "/etc/agntcy/dir/authz_policies.csv"

  # Store settings for the storage backend.
  store:
    # Storage provider to use.
    provider: "oci"

    # OCI-backed store
    oci:
      # Path to a local directory that will be to hold data instead of remote.
      # If this is set to non-empty value, only local store will be used.
      # local_dir: ""

      # Cache directory to use for metadata.
      # cache_dir: ""

      # Registry address to connect to
      registry_address: ""

      # All data will be stored under this repo.
      # Objects are pushed as tags, manifests, and blobs.
      repository_name: ""

      # Auth credentials to use.
      auth_config:
        insecure: true
        # username: ""
        # password: ""
        # access_token: ""
        # refresh_token: ""

  # Routing settings for the peer-to-peer network.
  routing:
    # Address to use for routing
    # listen_address: "/ip4/0.0.0.0/tcp/5555"

    # Path to private key file for peer ID.
    # key_path: /tmp/agntcy-dir/node.privkey

    # Nodes to use for bootstrapping of the DHT.
    # We read initial routing tables here and get introduced
    # to the network.
    # bootstrap_peers:
    #   - /ip4/1.1.1.1/tcp/1
    #   - /ip4/1.1.1.1/tcp/2

    # GossipSub configuration for efficient label announcements
    # When enabled, labels are propagated via GossipSub mesh to ALL subscribed peers
    # When disabled, falls back to DHT+Pull mechanism (higher bandwidth, limited reach)
    # Default: true (recommended for production)
    gossipsub:
      enabled: true

  # Sync configuration
  sync:
    # Authentication configuration for sync operations
    auth_config: {}
      # username: ""
      # password: ""

  # Events configuration
  events:
    # Channel buffer size per subscriber
    # Larger buffers allow subscribers to fall behind temporarily without dropping events
    # Default: 100
    subscriber_buffer_size: 100

    # Enable logging when events are dropped due to slow consumers
    # Default: true
    log_slow_consumers: true

    # Enable debug logging of all published events (verbose in production)
    # Default: false
    log_published_events: false

  # Publication configuration
  publication:
    # How frequently the scheduler checks for pending publications
    scheduler_interval: "1h"

    # Maximum number of publication workers running concurrently
    worker_count: 1

    # Timeout for individual publication operations
    worker_timeout: "30m"

  # gRPC Connection Management configuration
  # Protects server from resource exhaustion, zombie connections, and memory exhaustion
  # Production-safe defaults are applied automatically - customization is optional
  # Note: These settings can only be configured via Helm values (no environment variables)
  connection:
    # Connection limits
    # max_concurrent_streams: 1000    # Maximum concurrent gRPC streams per connection (default: 1000)
    # max_recv_msg_size: 4194304      # Maximum message size for receiving in bytes - 4MB (default: 4MB)
    # max_send_msg_size: 4194304      # Maximum message size for sending in bytes - 4MB (default: 4MB)
    # connection_timeout: 120s        # Timeout for establishing new connections (default: 120s)

    # Keepalive configuration - detects dead connections and prevents resource leaks
    # keepalive:
    #   max_connection_idle: 15m      # Close connections idle for this duration (default: 15m)
    #   max_connection_age: 30m       # Close connections after this age to rotate (default: 30m)
    #   max_connection_age_grace: 5m  # Grace period for in-flight RPCs before closing aged connections (default: 5m)
    #   time: 5m                      # Send keepalive pings every N duration (default: 5m)
    #   timeout: 1m                   # Close connection if ping not acknowledged within timeout (default: 1m)
    #   min_time: 1m                  # Minimum time between client pings (prevents abuse) (default: 1m)
    #   permit_without_stream: true   # Allow keepalive pings without active streams (default: true)

    # Example: High-traffic production configuration
    # connection:
    #   max_concurrent_streams: 2000
    #   max_recv_msg_size: 8388608    # 8MB for larger records
    #   max_send_msg_size: 8388608    # 8MB for larger records
    #   connection_timeout: 60s
    #   keepalive:
    #     max_connection_idle: 10m
    #     max_connection_age: 20m
    #     max_connection_age_grace: 3m
    #     time: 3m
    #     timeout: 30s
    #     min_time: 30s
    #     permit_without_stream: false

  # Rate limiting configuration
  # Protects the server from abuse and resource exhaustion using token bucket algorithm
  ratelimit:
    # Enable rate limiting middleware
    # Default: false (disabled for development/testing)
    enabled: false

    # Global rate limit (applies to all requests regardless of client)
    # Set both to 0 to disable global limiting
    # global_rps: 0       # Requests per second (float, e.g., 1000.0)
    # global_burst: 0     # Burst capacity (int, e.g., 2000)

    # Per-client rate limit (tracked by SPIFFE ID from mTLS)
    # Default values shown below are reasonable for production
    # Set both to 0 to disable per-client limiting
    per_client_rps: 100 # Requests per second per client (float)
    per_client_burst: 200 # Burst capacity per client (int)

    # Per-method rate limit overrides (optional)
    # Allows fine-grained control over specific gRPC methods
    # Note: These can only be configured via Helm values, not environment variables
    # method_limits:
    #   "/agntcy.dir.store.v1.StoreService/CreateRecord":
    #     rps: 50      # Lower limit for expensive operations
    #     burst: 100
    #   "/agntcy.dir.store.v1.StoreService/PullRecord":
    #     rps: 200     # Higher limit for read operations
    #     burst: 400

  # Database configuration (PostgreSQL)
  # Connection settings for the apiserver database.
  # Credentials are managed separately via secrets.postgresAuth or externalSecrets.
  database:
    # Database type (only postgres is supported)
    type: "postgres"

    # PostgreSQL connection settings
    postgres:
      # PostgreSQL host address
      # For local (postgresql.enabled=true): use "<release-name>-postgresql"
      # For external: your PostgreSQL hostname
      host: ""

      # PostgreSQL port
      port: 5432

      # Database name
      database: "dir"

      # SSL mode for PostgreSQL connection
      # Options: disable, require, verify-ca, verify-full
      ssl_mode: "disable"

  # OASF API validation configuration
  oasf_api_validation:
    # Schema URL for OASF validation
    # Used by both the apiserver and reconciler for validating agent records
    schema_url: "https://schema.oasf.outshift.com"

    # Disable OASF validation entirely (default: false)
    # Set to true to skip schema validation (useful for testing)
    disable: false

# SPIRE configuration
spire:
  enabled: false
  trustDomain: example.org

  # SPIRE controller className for ClusterSPIFFEID matching
  #
  # REQUIRED: The className field is mandatory for ClusterSPIFFEID resources.
  # The SPIRE controller manager uses className to match ClusterSPIFFEID resources
  # with the appropriate SPIRE installation. Without this field, the controller
  # will ignore the ClusterSPIFFEID and no workload registration will occur,
  # causing authentication failures and preventing the gRPC server from starting.
  #
  # The className must match the SPIRE installation's className.
  #
  # Default: "dir-spire" (matches standard SPIRE installation convention).
  # If your SPIRE installation uses a different className, override this value.
  # If not specified, falls back to "<namespace>-spire" (e.g., "my-namespace-spire").
  #
  # IMPORTANT: Ensure this matches your SPIRE Controller Manager's className.
  # For standard installations, SPIRE is deployed in the "spire" namespace with
  # className "dir-spire". Verify with:
  #   kubectl get deployment -n spire spire-server -o yaml | grep className
  #
  # See: https://github.com/spiffe/spire-controller-manager/blob/main/docs/clusterspiffeid-crd.md
  className: "dir-spire"

  # Use SPIFFE CSI driver for workload attestation (recommended)
  #
  # When true (recommended):
  #   - Uses SPIFFE CSI driver for proper workload registration
  #   - Workload registration happens synchronously before pod starts
  #   - SPIRE agent issues X.509-SVID with URI SAN before pod begins
  #   - Eliminates authentication delays and race conditions
  #   - Production-ready, reliable identity injection
  #
  # When false (legacy/debugging only):
  #   - Uses hostPath to mount SPIRE agent socket
  #   - Workload registration happens asynchronously via ClusterSPIFFEID
  #   - Pod may retry authentication during startup
  #   - Only use for debugging or when CSI driver is unavailable
  #
  # Requires: SPIFFE CSI driver deployed in cluster
  # See: https://github.com/spiffe/spiffe-csi
  useCSIDriver: true

  # Custom DNS names to add to the X.509-SVID certificate SANs
  # Useful for external access with proper TLS verification without --tls-skip-verify
  # Example:
  # dnsNameTemplates:
  #   - "dir-api.example.com"
  #   - "api.example.com"
  dnsNameTemplates: []

  federation: []
    # # Config: https://github.com/spiffe/spire-controller-manager/blob/main/docs/clusterfederatedtrustdomain-crd.md
    # - trustDomain: dir-cluster
    #   bundleEndpointURL: https://0.0.0.0:8081
    #   bundleEndpointProfile:
    #     type: https_web

# Create PVC for routing/cache data
# IMPORTANT: When PVC is enabled, set strategy.type to "Recreate" to avoid
# BadgerDB lock conflicts during updates (see strategy configuration above)
pvc:
  create: false
  storageClassName: standard
  size: 1G

# Service exposes gRPC server api
service:
  type: ClusterIP
  port: 8888

# Prometheus metrics configuration
# This section configures BOTH the application metrics server AND Kubernetes service
# The values are automatically injected into the server configuration
metrics:
  # Enable Prometheus metrics collection
  # Default: true (recommended for production)
  enabled: true

  # Port for Prometheus metrics endpoint
  # Used for both the application listen address and Kubernetes service port
  # Default: 9090
  port: 9090

  # ServiceMonitor for Prometheus Operator (optional)
  # Creates a ServiceMonitor resource for automatic Prometheus discovery
  serviceMonitor:
    enabled: false
    interval: 30s
    scrapeTimeout: 10s
    labels: {}

# Routing service exposes P2P networking (separate from API service)
routingService:
  # Service type for routing/P2P traffic
  # Options: ClusterIP, NodePort, LoadBalancer
  # Default: NodePort (works everywhere - local Kind and cloud)
  # For production cloud: override to LoadBalancer for stable external IP
  type: NodePort

  # Cloud provider for automatic annotation configuration
  # Options: "aws", "gcp", "azure", or leave empty for manual configuration
  # When set, provider-specific annotations are automatically applied
  cloudProvider: ""

  # AWS-specific configuration (only used when cloudProvider: "aws")
  aws:
    # Use internal load balancer (default: false = internet-facing)
    internal: false
    # NLB target type: "instance" or "ip" (default: instance)
    # nlbTargetType: "instance"

  # GCP-specific configuration (only used when cloudProvider: "gcp")
  gcp:
    # Use internal load balancer (default: false = external)
    internal: false
    # Optional: BackendConfig name for advanced configuration
    # backendConfig: ""

  # Azure-specific configuration (only used when cloudProvider: "azure")
  azure:
    # Use internal load balancer (default: false = public)
    internal: false
    # Optional: Resource group for load balancer
    # resourceGroup: ""

  # Optional: Specify a static IP (must be reserved in cloud provider first)
  # loadBalancerIP: ""

  # Optional: Preserve client source IPs (recommended for P2P)
  externalTrafficPolicy: Local

  # Optional: Fixed NodePort (only used when type is NodePort)
  # nodePort: 30555

  # Optional: Additional custom annotations (merged with provider annotations)
  # Custom annotations take precedence over provider-generated ones
  annotations: {}

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  replicaCount: 1
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Deployment strategy for pod updates
# IMPORTANT: When using PVCs for routing, use "Recreate" strategy
# to avoid file lock conflicts with BadgerDB.
#
# Recreate: Terminates old pod before starting new one (default, required with PVCs)
# - Ensures clean database shutdown and no lock conflicts
# - Brief downtime during updates (10-15 seconds)
# - Prevents CrashLoopBackOff due to file locks
#
# RollingUpdate: Zero-downtime updates (only for stateless deployments without routing PVC)
# - NOT recommended when routing PVC is enabled
# - Can cause BadgerDB lock conflicts
#
# To use RollingUpdate (only if NO routing PVC is used):
# strategy:
#   type: RollingUpdate
#   rollingUpdate:
#     maxSurge: 1
#     maxUnavailable: 0
strategy:
  type: Recreate

nodeSelector: {}

tolerations: []

affinity: {}

extraVolumes: []
  # Example:
  # - name: datastore
  #   configMap:
  #     name: my-configmap

extraVolumeMounts: []
  # Example:
  # - name: datastore
  #   mountPath: /etc/datastore

# Extra environment variables for the apiserver container
extraEnv: []
# - name: SSL_CERT_DIR
#   value: "/etc/ca-certs"
# - name: GOCOVERDIR
#   value: /tmp/coverage

revisionHistoryLimit: 2

# Secrets configuration
# Choose ONE of two methods:
# 1. Helm-managed secrets (secrets.*) - credentials in values.yaml
# 2. ExternalSecrets (externalSecrets.*) - credentials synced from Vault
#
# Method 1: Helm-managed secrets (default)
# Sensitive credentials are stored in Kubernetes secrets and injected as environment variables
secrets:
  # Private key for peer-to-peer routing identity
  # If not provided, the secret will not include this key
  privKey: ""

  # Sync authentication credentials
  # Used for authenticating sync operations between nodes
  # Username defaults to "sync" if empty, password is randomly generated if empty
  syncAuth:
    username: ""
    password: ""

  # OCI (Open Container Initiative) registry authentication
  # Used for authenticating to the OCI-backed storage backend
  # Username defaults to "admin" if empty, password is randomly generated if empty
  ociAuth:
    username: ""
    password: ""

  # PostgreSQL authentication
  # These credentials are used for database connections.
  # When postgresql subchart is enabled, these values override postgresql.auth.* for the connection secret.
  # Username defaults to "dir", password is randomly generated if empty.
  postgresAuth:
    username: ""
    password: ""

# Method 2: ExternalSecrets configuration
# Syncs credentials from HashiCorp Vault (or other secret providers) using External Secrets Operator
# When enabled, the Helm-managed secret (above) is NOT created
externalSecrets:
  # Enable ExternalSecrets integration (default: false)
  # When true, credentials are synced from Vault instead of using values.yaml
  enabled: false

  # Vault path where credentials are stored (all credentials in one path)
  # Example: "dir_staging/dev/credentials"
  vaultPath: ""

  # ClusterSecretStore or SecretStore name to use
  # This must be pre-configured in your cluster (managed by platform team)
  secretStore: "vault-backend"

  # Secret store kind (default: ClusterSecretStore)
  # Options: ClusterSecretStore (cluster-wide) or SecretStore (namespace-scoped)
  secretStoreKind: "ClusterSecretStore"

  # Refresh interval - how often ESO syncs from Vault (default: 1h)
  refreshInterval: "1h"

  # Node identity configuration
  nodeIdentity:
    enabled: true
    # Property name in Vault secret (default: "node.privkey")
    property: "node.privkey"

  # OCI registry authentication
  ociAuth:
    enabled: true
    # Property names in Vault secret (defaults shown)
    usernameProperty: "oci-username"
    passwordProperty: "oci-password"

  # Sync authentication (shared with remote nodes)
  syncAuth:
    enabled: true
    # Property names in Vault secret (defaults shown)
    usernameProperty: "sync-username"
    passwordProperty: "sync-password"
  
  # PostgreSQL authentication (only used when database.type: "postgres")
  postgresAuth:
    enabled: true 
    # Property names in Vault secret (defaults shown)
    usernameProperty: "postgres-username"
    passwordProperty: "postgres-password"

# OASF server subchart configuration (OPTIONAL)
# OASF is NOT installed by default. Set enabled: true to deploy an OASF schema server instance.
oasf:
  enabled: false

# Envoy auth gateway subchart configuration (OPTIONAL)
# Provides GitHub authentication via Envoy gateway with ext_authz
# NOT installed by default. Set enabled: true to deploy.
# Section 1: Enable/disable the subchart
envoyAuthz:
  enabled: false

# Section 2: Configuration values for the envoy-authz subchart
# NOTE: Only takes effect when envoyAuthz.enabled: true
envoy-authz:
  # Envoy gateway configuration
  envoy:
    replicaCount: 2

    # Backend Directory API address (required when enabled)
    # Set to the full service name of Directory API
    backend:
      address: "" # e.g., dir-dir-dev-argoapp-apiserver.dir-dev-dir.svc.cluster.local
      port: 8888

    # SPIFFE integration for mTLS with Directory
    spiffe:
      enabled: true
      # trustDomain: example.org
      # className: dir-spire

  # Authorization server configuration
  authServer:
    replicaCount: 2

    # Image configuration
    # Override these values to use custom images (e.g., development branches)
    # image:
    #   repository: ghcr.io/agntcy/envoy-authz-dev
    #   tag: feat-feat-github-auth-ad4a58e
    #   pullPolicy: IfNotPresent

    # Authorization rules
    authorization:
      # Allowed GitHub organizations
      # Empty list = allow all authenticated users
      allowedOrgConstructs: []
      # Example: ["agntcy", "spiffe"]

      # Explicitly allowed users (format: "github:username")
      userAllowList: []
      # Example: ["github:tkircsi"]

      # Explicitly denied users (format: "github:username")
      userDenyList: []
      # Example: ["github:malicious-user"]

    # GitHub provider configuration
    github:
      enabled: true
      cacheTTL: 5m
      apiTimeout: 10s

  # Ingress configuration (optional - for external access)
  ingress:
    enabled: false
    className: nginx
    host: "" # e.g., gateway.dev.ads.outshift.io
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt
      external-dns.alpha.kubernetes.io/hostname: ""
      # gRPC backend configuration (required for dirctl client over HTTPS)
      nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
      nginx.ingress.kubernetes.io/grpc-backend: "true"

# PostgreSQL subchart configuration
# Deploys PostgreSQL alongside the apiserver for dev/production use.
# Set enabled: false when using SQLite or external PostgreSQL.
# Note: database.type (above) controls which backend the apiserver uses;
#       postgresql.enabled controls whether to deploy this subchart.
# When enabled, postgresql.auth.* is the single source of truth for credentials.
postgresql:
  enabled: true
  # image:
  #   registry: docker.io
  #   repository: bitnami/postgresql
  #   digest: sha256:c47e54a69b39aa97d4405d68ea02bf2ffe06b646748e2ddf9c761416ead6acd5
  auth:
    username: "dir"
    password: "dirpassword"
    database: "dir"

# Reconciler configuration
# The reconciler handles async operations including:
# - regsync: syncs records from remote registries
# - indexer: indexes synced records into the database for search
# Shares the same database as the apiserver.
reconciler:
  image:
    repository: ghcr.io/agntcy/dir-reconciler
    tag: latest
    pullPolicy: IfNotPresent

  # Reconciler configuration - all settings go here
  # This entire section is rendered directly to reconciler.config.yml
  config:
    # Database configuration (PostgreSQL)
    # Connection settings for the apiserver database.
    # Credentials are managed separately via secrets.postgresAuth or externalSecrets.
    database:
      # Database type (only postgres is supported)
      type: "postgres"

      # PostgreSQL connection settings
      postgres:
        # PostgreSQL host address
        # For local (postgresql.enabled=true): use "<release-name>-postgresql"
        # For external: your PostgreSQL hostname
        host: ""

        # PostgreSQL port
        port: 5432

        # Database name
        database: "dir"

        # SSL mode for PostgreSQL connection
        # Options: disable, require, verify-ca, verify-full
        ssl_mode: "disable"

    # Local registry configuration (for syncing)
    local_registry:
      # Path to a local directory that will be to hold data instead of remote.
      # If this is set to non-empty value, only local store will be used.
      # local_dir: ""

      # Cache directory to use for metadata.
      # cache_dir: ""

      # Registry address to connect to
      registry_address: ""

      # All data will be stored under this repo.
      # Objects are pushed as tags, manifests, and blobs.
      repository_name: ""

      # Auth credentials to use.
      auth_config:
        insecure: true
        # username: ""
        # password: ""
        # access_token: ""
        # refresh_token: ""

    # OASF schema URL for validation
    schema_url: "https://schema.oasf.outshift.com"

    # Regsync task configuration
    regsync:
      enabled: true
      interval: "1h"
      timeout: "10m"

    # Indexer task configuration
    indexer:
      enabled: true
      interval: "1h"

  resources: {}
